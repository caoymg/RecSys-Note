### 1. ğŸš— Probabilistic Matrix Factorization (PMF) modelåŸºäºæ¦‚ç‡çš„çŸ©é˜µåˆ†è§£æ¨¡å‹
1.1 **Preliminaries**
#### low-dimensional factor models

- The idea behind such models is that attitudes or preferences of a user are determined by a small number of unobserved factors

- for N users and M movies, the N Ã—M preference matrix R is given by the product of an N Ã—D user coefficient matrix U T and a D Ã— M factor matrix V. Training such a model amounts to finding the best rank(ç§©)-D approximation to the observed N Ã— M target matrix R under the given loss function.

#### probabilistic factor-based models

- can be viewed as graphical models in which hidden factor variables have directed connections to variables that represent user ratings.
- The major drawback of such models is that exact inference is intractable, which means that potentially slow or inaccurate approximations are required for **computing the posterior distribution(åéªŒæ¦‚ç‡ï¼ŒçŸ¥æœæ±‚å› )** over hidden factors in such models.è¿™ç±»æ¨¡å‹çš„ä¸»è¦ç¼ºç‚¹æ˜¯å¾ˆéš¾è¿›è¡Œç²¾ç¡®çš„æ¨æ–­ï¼Œè¿™æ„å‘³ç€åœ¨è®¡ç®—æ­¤ç±»æ¨¡å‹ä¸­éšè—å› ç´ çš„åéªŒåˆ†å¸ƒæ—¶ï¼Œéœ€è¦æ½œåœ¨çš„ç¼“æ…¢æˆ–ä¸å‡†ç¡®çš„è¿‘ä¼¼ã€‚

#### Singular Value Decomposition (SVD)

- SVD finds the approximation matrix RË† = U *T* V of the given rank which minimizes the sum-squared distance to the target matrix R. 

- Since most real-world datasets are sparse, most entries in R will be missing. In those cases, the sum-squared distance is computed only for the observed entries of the target matrix R.

- Seemingly minor modification results in a difficult **non-convex optimization problem** which cannot be solved using standard SVD implementations.è¿™ä¸ªçœ‹ä¼¼å¾ˆå°çš„ä¿®æ”¹å¯¼è‡´äº†ä¸€ä¸ªå›°éš¾çš„éå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œè€Œè¿™ä¸ªé—®é¢˜æ— æ³•ç”¨æ ‡å‡†çš„SVDå®ç°æ¥è§£å†³ã€‚

  ï¼ˆå‡¸ä¼˜åŒ–ä»»ä½•å±€éƒ¨æœ€ä¼˜è§£å³ä¸ºå…¨å±€æœ€ä¼˜è§£ã€‚ç”±äºè¿™ä¸ªæ€§è´¨ï¼Œåªè¦è®¾è®¡ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„å±€éƒ¨ç®—æ³•ï¼Œä¾‹å¦‚è´ªå©ªç®—æ³•ï¼ˆGreedy Algorithmï¼‰æˆ–æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Decentï¼‰ï¼Œæ”¶æ•›æ±‚å¾—çš„å±€éƒ¨æœ€ä¼˜è§£å³ä¸ºå…¨å±€æœ€ä¼˜ã€‚è€Œéå‡¸ä¼˜åŒ–é—®é¢˜è¢«è®¤ä¸ºæ˜¯éå¸¸éš¾æ±‚è§£çš„ï¼Œå› ä¸ºå¯è¡ŒåŸŸé›†åˆå¯èƒ½å­˜åœ¨æ— æ•°ä¸ªå±€éƒ¨æœ€ä¼˜ç‚¹ï¼Œé€šå¸¸æ±‚è§£å…¨å±€æœ€ä¼˜çš„ç®—æ³•å¤æ‚åº¦æ˜¯æŒ‡æ•°çº§çš„ã€‚ï¼‰

- Instead of constraining the rank of the approximation matrix RË† = U T V , i.e. the number of factors, [10] proposed penalizing the norms of U and V . Learning in this model, however, requires solving a **sparse semi-definite program (SDP) åŠæ­£å®šè§„åˆ’**, making this approach infeasible for datasets containing millions of observations.æƒ©ç½šUå’ŒVçš„èŒƒæ•°ï¼Œè€Œä¸æ˜¯é™åˆ¶è¿‘ä¼¼çŸ©é˜µRË†= U T Vçš„ç§©ï¼Œå³å› å­çš„æ•°é‡ã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸ªæ¨¡å‹ä¸­å­¦ä¹ éœ€è¦æ±‚è§£ä¸€ä¸ªç¨€ç–åŠå®šç¨‹åº(SDP)ï¼Œè¿™ä½¿å¾—è¿™ç§æ–¹æ³•å¯¹äºåŒ…å«æ•°ç™¾ä¸‡ä¸ªè§‚æµ‹æ•°æ®é›†ä¸å¯è¡Œã€‚

â¬†ï¸none of these methods have proved to be particularly successful for two reasons. 

- First, none of the above-mentioned approaches, except for the matrix-factorization-based ones, scale well to large datasets.
- Second, most of the existing algorithms have trouble making accurate predictions for users who have very few ratings

A common practice in the collaborative filtering community is to **remove all users with fewer than some minimal number of ratingsï¼ˆåˆ é™¤æ‰€æœ‰è¯„çº§æ•°å°‘äºæœ€ä½çº¿çš„ç”¨æˆ·ï¼‰**.



#### Probabilistic Matrix Factorization (PMF)

Suppose we have M movies, N users, and integer rating values from 1 to K1. Let Rij represent the rating of user i for movie j, U âˆˆ RDÃ—N and V âˆˆ RDÃ—M be latent user and movie feature matrices, with column vectors Ui and Vj representing user-specific and movie-specific latent feature vectors respectively. 

åŸºç¡€PMFæ¨¡å‹, ä½¿ç”¨å¦‚ä¸‹ä¸¤ä¸ªå‡è®¾:

(1) è§‚æµ‹å™ªå£°ï¼ˆè§‚æµ‹è¯„åˆ†çŸ©é˜µR å’Œè¿‘ä¼¼è¯„åˆ†çŸ©é˜µR ^ä¹‹å·®ï¼‰ä¸ºé«˜æ–¯åˆ†å¸ƒ
(2) ç”¨æˆ·å±æ€§U å’Œç”µå½±å±æ€§V å‡ä¸ºé«˜æ–¯åˆ†å¸ƒ

ç”±äºæ¨¡å‹æ€§èƒ½æ˜¯é€šè¿‡è®¡ç®—æµ‹è¯•é›†ä¸Šçš„å‡æ–¹æ ¹è¯¯å·®(RMSE)æ¥è¡¡é‡çš„ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨å¸¦æœ‰é«˜æ–¯è§‚æµ‹å™ªå£°çš„æ¦‚ç‡çº¿æ€§æ¨¡å‹ Since model performance is measured by computing the root mean squared error (RMSE) on the test set we first adopt a probabilistic linear model with Gaussian observation noise. We define the conditional distribution over the observed ratings as

![img](https://p26-tt.byteimg.com/origin/pgc-image/5315d4be56b0487792d927d5d276d22b)

- N (x|Âµï¼ŒÏƒ2)æ˜¯å‡å€¼ä¸ºÂµï¼Œæ–¹å·®ä¸ºÏƒ2çš„é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°  N (x|Âµ, Ïƒ2) is the probability density function of the Gaussian distribution with mean Âµ and variance Ïƒ2

- Iijæ˜¯ç”¨æˆ·iè¯„ä»·ç”µå½±jæ—¶ç­‰äº1ï¼Œå¦åˆ™ç­‰äº0çš„æŒ‡æ ‡å‡½æ•° Iij is the indicator function that is equal to 1 if user i rated movie j and equal to 0 otherwise. 



åœ¨ç”¨æˆ·å’Œç”µå½±ç‰¹å¾å‘é‡ä¸Šæ”¾ç½®äº†é›¶å‡å€¼çƒé¢é«˜æ–¯å…ˆéªŒ: place **zero-mean spherical Gaussian priors** on **user** and **movie feature vectors**:

![img](https://p1-tt-ipv6.byteimg.com/origin/pgc-image/4475b390e8a34c5a8c2e9c69effd7542)



ç”¨æˆ·ç‰¹å¾å’Œç”µå½±ç‰¹å¾çš„åéªŒåˆ†å¸ƒçš„å¯¹æ•°ä¸ºThe log of the **posterior distribution** over the **user** and **movie features** is given by


![img](https://inews.gtimg.com/newsapp_ls/0/13255827617/0)

- C is a constant that does not depend on the parameters.



åœ¨ä¿æŒå›ºå®šçš„è¶…å‚æ•°(å³è§‚æµ‹å™ªå£°æ–¹å·®å’Œå…ˆéªŒæ–¹å·®)ä¸‹ï¼Œæœ€å¤§åŒ–ç”µå½±å’Œç”¨æˆ·ç‰¹å¾çš„å¯¹æ•°åéªŒç­‰ä»·äºä½¿ç”¨äºŒæ¬¡æ­£åˆ™åŒ–é¡¹ä½¿è¯¯å·®å¹³æ–¹å’Œæœ€å°:   Maximizing the log-posterior over movie and user features with hyperparameters (i.e. the observation noise variance and prior variances) kept fixed is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms:


![img](https://p9-tt-ipv6.byteimg.com/origin/pgc-image/11428fd10d86432895e8c4d54ea04c4e)

- Î»Uè¡¨ç¤ºFrobeniusèŒƒæ•° Î»U denotes the Frobenius norm
- é€šè¿‡å¯¹Uå’ŒVè¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå¯ä»¥æ‰¾åˆ°ç”±å¼4ç»™å‡ºçš„ç›®æ ‡å‡½æ•°çš„å±€éƒ¨æœ€å°å€¼ã€‚ A local minimum of the objective function given by Eq. 4 can be found by performing gradient descent in U and V .
- è¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹å¯ä»¥è¢«è§†ä¸ºSVDæ¨¡å‹çš„æ¦‚ç‡æ‰©å±•ï¼Œå› ä¸ºå¦‚æœè§‚å¯Ÿåˆ°æ‰€æœ‰è¯„çº§ï¼Œåˆ™åœ¨å…ˆéªŒæ–¹å·®è¶‹äºæ— ç©·å¤§çš„æé™ä¸‹ï¼ŒEq. 4ç»™å‡ºçš„ç›®æ ‡å°†å‡å°‘ä¸ºSVDç›®æ ‡. Note that this model can be viewed as a **probabilistic extension of the SVD model**, since if all ratings have been observed, the objective given by Eq. 4 reduces to the SVD objective in the limit of prior variances going to infinity.

åœ¨æˆ‘ä»¬çš„å®éªŒä¸­, æ²¡æœ‰ä½¿ç”¨ä¸€ä¸ªç®€å•çš„linear-Gaussianæ¨¡å‹æ¥é¢„æµ‹æœ‰æ•ˆè¯„çº§çš„èŒƒå›´ä»¥å¤–çš„å€¼ In our experiments, instead of using a simple linear-Gaussian model, which can make predictions outside of the range of valid rating values

ç”¨æˆ·å’Œç”µå½±çš„ç‰¹å¾å‘é‡ä¹‹é—´çš„ç‚¹ç§¯é€šè¿‡é€»è¾‘å‡½æ•°g (x) = 1 / (1 + exp(âˆ’x))ï¼Œå®ç°è¾¹ç•ŒèŒƒå›´çš„é¢„æµ‹ the dot product between user- and movie-specific feature vectors is passed through the logistic function g(x) = 1/(1 + exp(âˆ’x)), which bounds the range of predictions

![img](https://p1-tt-ipv6.byteimg.com/origin/pgc-image/419d5121d51248ddab50aaecdc03e9fe)

We map the ratings 1, ..., K to the interval [0, 1] using the function t(x) = (x âˆ’ 1)/(K âˆ’ 1), so that the range of valid rating values matches the range of predictions our model makes. Minimizing the objective function given above using steepest descent takes time linear in the number of observations. 


